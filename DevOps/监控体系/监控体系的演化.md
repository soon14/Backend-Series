[![](https://i.postimg.cc/WzXsh0MX/image.png)](https://github.com/wx-chevalier/Backend-Series)

# 传统监控

## Zabbix

Zabbix 是早期的选择，简单的配置页面，丰富的 agent 数据采集，支持短信、邮件及微信告警，在一个星期内，我们就完成了全站的基础监控。Zabbix 开箱即用的使用方式，适合初创型公司。即使是现在，Zabbix 还在线上运行，监控网络设备的运行状态。随着业务高速发展，研发的需求越来越复杂，同时也暴露出 Zabbix 的很多缺点。

- Zabbix 性能瓶颈，监控数据存储在 Mysql 中，随着监控数据越来越多，Zabbix 响应时间变慢。

- Zabbix 只支持 metric 类型监控，对于日志类监控，支持并不友好。

- Zabbix 监控大盘页面不美观，无法满足业务方定制的需求。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f16geqrtj30go08cq3i.jpg)

![Zabbix 组件结构](https://s2.ax1x.com/2019/10/02/udoh8J.png)

Zabbix是由Alexei Vladishev开源的分布式监控系统，支持多种采集方式和采集客户端，同时支持SNMP、IPMI、JMX、Telnet、SSH等多种协议，它将采集到的数据存放到数据库中，然后对其进行分析整理，如果符合告警规则，则触发相应的告警。

Zabbix核心组件主要是Agent和Server，其中Agent主要负责采集数据并通过主动或者被动的方式采集数据发送到Server/Proxy，除此之外，为了扩展监控项，Agent还支持执行自定义脚本。Server主要负责接收Agent发送的监控信息，并进行汇总存储，触发告警等。

为了便于快速高效的配置Zabbix监控项，Zabbix提供了模板机制，从而实现批量配置的目的。

Zabbix Server将收集的监控数据存储到Zabbix Database中。Zabbix Database支持常用的关系型数据库，例如MySQL、PostgreSQL、Oracle等，默认MySQL。Zabbix Web页面（PHP编写）负责数据查询。Zabbix由于使用了关系型数据存储时序数据，所以在监控大规模集群时常常在数据存储方面捉襟见肘。为此Zabbix 4.2版本后也开始支持时序数据存储，不过目前还不成熟。

# 数据采集

## CAT

CAT (Central Application Tracking) 是基于 Java 开发的实时监控平台，主要包括移动端监控，应用侧监控，核心网络层监控，系统层监控等。

CAT 的优点是功能丰富，支持钉钉告警，95 线 99 线计算，可展示代码级别监控，在代码层故障定位提供了强有力的工具。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f16geqrtj30go08cq3i.jpg)

## LEPUS

Lepus(天兔) 数据库企业监控系统是一套由专业 DBA 针对互联网企业开发的一款强大的企业数据库监控管理系统。Lepus 后端采用 Python 语言开发，对于运维非常友好，可以很方便地作出一些个性化的修改。

Lepus 的优点是无需安装 agent，账号集中管理，适合作为数据库的 CMDB 使用。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f16geqrtj30go08cq3i.jpg)

## ELK 监控生态

ELK (Elasticsearch,Logstash,Kibana) 是 Elastic 公司提供的三个开源组件。在日常工作中，我们需要进行日志分析场景：直接对日志文件进行 grep、awk 等正则操作，获取我们想要的信息。在大规模的场景中，日志文件分布在不同的服务器上，且文件非常大，逐台操作性能非常低。比如 Nginx 日志，Mysql 慢查询日志，应用 log 日志等。ELK 提供一整套的解决方案，可以帮助我们快速全站查询。

下图是 Mysql 慢查询的截图，通过 Python 脚本，可以实时读取 Mysql 慢查询日志，并写入 ES，方便查看线上问题。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f3f3fcn3j30gm07qq3f.jpg)

下图是服务器的 dashboard，通过模糊匹配，可以快速查询相关服务器组的性能指标。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f3f3fcn3j30gm07qq3f.jpg)

## Open-Falcon

Open-Falcon 是小米开源的监控系统，灵活的数据采集，水平扩展能力以及高效的告警策略帮助我们快速监控 servers 的信息。在实际的环境中，我们仅采用了 falcon-agent、falcon-transfer 组件，帮助我们采集数据，具体的存储及展示由更专业的组件处理。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f3i634lsj30u00jwabs.jpg)

![Open Falcon  组件架构](https://s2.ax1x.com/2019/10/02/udT9qP.jpg)

Open-Falcon是小米开源的企业级监控工具，用Go语言开发而成，包括小米、滴滴、美团等在内的互联网公司都在使用它，是一款灵活、可扩展并且高性能的监控方案，主要组件包括了：

- Falcon-agent：用Go语言开发的Daemon程序，运行在每台Linux服务器上，用于采集主机上的各种指标数据，主要包括CPU、内存、磁盘、文件系统、内核参数、Socket连接等，目前已经支持200多项监控指标。并且，Agent支持用户自定义的监控脚本。

- Hearthbeat server：简称HBS心跳服务，每个Agent都会周期性地通过RPC方式将自己的状态上报给HBS，主要包括主机名、主机IP、Agent版本和插件版本，Agent还会从HBS获取自己需要执行的采集任务和自定义插件。

- Transfer：负责接收Agent发送的监控数据，并对数据进行整理，在过滤后通过一致性Hash算法发送到Judge或者Graph。

- Graph：RRD数据上报、归档、存储的组件。Graph在收到数据以后，会以RRDtool的数据归档方式来存储，同时提供RPC方式的监控查询接口。

- Judge：告警模块，Transfer转发到Judge的数据会触发用户设定的告警规则，如果满足，则会触发邮件、微信或者回调接口。这里为了避免重复告警引入了Redis暂存告警，从而完成告警的合并和抑制。

- Dashboard：面向用户的监控数据查询和告警配置界面。

## 综合对比

从开发语言上看，为了应对高并发和快速迭代的需求，监控系统的开发语言已经慢慢从C语言转移到Go。不得不说，Go凭借简洁的语法和优雅的并发，在Java占据业务开发，C占领底层开发的情况下，准确定位中间件开发需求，在当前开源中间件产品中被广泛应用。

从系统成熟度上看，Zabbix是老牌的监控系统：Zabbix是在1998年出现的，系统功能比较稳定，成熟度较高。而Prometheus和Open-Falcon都是最近几年才诞生的，虽然功能还在不断迭代更新，但站在巨人的肩膀之上，在架构设计上借鉴了很多老牌监控系统的经验。

从系统扩展性方面看，Zabbix和Open-Falcon都可以自定义各种监控脚本，并且Zabbix不仅可以做到主动推送，还可以做到被动拉取，Prometheus则定义了一套监控数据规范，并通过各种exporter扩展系统采集能力。

从数据存储方面来看，Zabbix采用关系数据库保存，这极大限制了Zabbix采集的性能，Open-Falcon采用RDD数据存储，并且可以对接到OpenTSDB，而Prometheus自研一套高性能的时序数据库，在V3版本可以达到每秒千万级别的数据存储，通过对接第三方时序数据库扩展历史数据的存储。

从配置和维护的复杂度上看，Prometheus只有一个核心server组件，一条命令便可以启动，相比而言，其他系统配置相对麻烦，尤其是Open-Falcon。

从社区活跃度上看，目前Zabbix社区活跃度比较低，Open-Falcon虽然也比较活跃，但基本都是国内的公司参与，Prometheus在这方面占据绝对优势，社区活跃度最高，并且受到CNCF的支持，后期的发展值得期待。

从容器支持角度看，由于Zabbix出现得比较早，当时容器还没有诞生，自然对容器的支持也比较差。Open-Falcon虽然提供了容器的监控，但支持力度有限。Prometheus的动态发现机制，不仅可以支持Swarm原生集群，还支持Kubernetes容器集群的监控，是目前容器监控最好解决方案。Zabbix在传统监控系统中，尤其是在服务器相关监控方面，占据绝对优势。伴随着容器的发展，Prometheus开始成为主导及容器监控方面的标配，并且在未来可见的时间内被广泛应用。总体来说，对比各种监控系统的优劣，Prometheus可以说是目前监控领域最锋利的“瑞士军刀”了。

# Prometheus 

Prometheus 是一套开源的监控、报警、时间序列数据库的组合；它的灵感来自谷歌的Borgmon，一个实时的时间序列监控系统，Borgmon 使用这些时间序列数据来识别问题并发出警报。Prometheus 最初由前谷歌SRE Matt T. Proud开发，并转为一个研究项目。在Proud加入SoundCloud之后，他与另一位工程师Julius Volz合作开发了Prometheus。后来其他开发人员陆续加入了这个项目，并在SoundCloud内部继续开发，最终于2015年1月公开发布。

起始是由 SoundCloud 公司开发的。随着发展，越来越多公司和组织接受采用 Prometheus，社区也十分活跃，他们便将它独立成开源项目，并且有公司来运作。google SRE 的书内也曾提到跟他们 BorgMon 监控系统相似的实现是 Prometheus。现在最常见的 Kubernetes 容器管理系统中，通常会搭配 Prometheus 进行监控。

Prometheus 的优势在于其易于安装使用，外部依赖较少；并且直接按照分布式、微服务架构模式进行设计，支持服务自动化发现与代码集成。Prometheus 能够自定义多维度的数据模型，内置强大的查询语句，搭配其丰富的社区扩展，能够轻松实现数据可视化。

![](https://i.postimg.cc/g0SDCRhK/image.png)

Prometheus由两个部分组成，一个是监控报警系统，另一个是自带的时序数据库（TSDB）。上图是Prometheus整体架构图，左侧是各种符合Prometheus数据格式的exporter，除此之外为了支持推动数据类型的Agent，可以通过Pushgateway组件，将Push转化为Pull。Prometheus甚至可以从其它的Prometheus获取数据，组建联邦集群。Prometheus的基本原理是通过HTTP周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口并且符合Prometheus定义的数据格式，就可以接入Prometheus监控。

![组件连接](https://s2.ax1x.com/2019/10/02/udBOyj.jpg)

上侧是服务发现，Prometheus支持监控对象的自动发现机制，从而可以动态获取监控对象。图片中间是Prometheus Server，Retrieval模块定时拉取数据，并通过Storage模块保存数据。PromQL为Prometheus提供的查询语法，PromQL模块通过解析语法树，调用Storage模块查询接口获取监控数据。图片右侧是告警和页面展现，Prometheus将告警推送到alertmanger，然后通过alertmanger对告警进行处理并执行相应动作。数据展现除了Prometheus自带的WebUI，还可以通过Grafana等组件查询Prometheus监控数据。


# 数据存储

关于采集到的数据如何存储，主流的选择是将数据写入到一个时序数据库中。

- Prometheus 提供了丰富的数据模型和查询语句，容易上手，很容易集成到现有的环境中，但是 Prometheus 的集群和 HA 架构并不成熟，需要额外的开发，并不适合。

- InfluxDB 是在 Prometheus 之后才提出的，并且提供商业的伸缩和集群化服务，相比 Prometheus 的 metrics 存储，InfluxDB 还能处理事件类型的数据，对于大部分公司而言，商业化基本不会考虑。

- OpenTSDB 是一个基于 Hadoop 和 Hbase 的分布式事件序列数据库，相比 Prometheus 和 InfluxDB，OpenTSDB 的横向扩缩容很容易 (需要有丰富的 Hadoop/HBase 维护经验), 同时官方 Open-falcon 支持 OpenTSDB。

在 Prometheus 中收集所有指标后，你可以使用 Grafana 可视化这些指标。关于数据展示的选型，在没有自研能力的情况下，Grafana 是不二选择。Grafana 的告警功能强大方便，同时支持钉钉，Webhook 等，满足公司所有的需求。与此同时，我们将 Grafana 和 Docker 技术结合，实现了 Grafana 高可用、自愈和无限扩展能力。

# AIOps

在数据采集阶段，保留了 Open-Faclon、CAT、 客户端 SDK、Logstash 等入口，通过 Kafka 进行汇聚，引入大数据实时计算平台 Flink，提炼 metric 指标，并最终入库。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g0f3i634lsj30u00jwabs.jpg)

在告警方面，开发了 Alert Manager 组件，对接多种告警渠道：钉钉、短信、微信等，并且自动创建 Jira，告警闭环。
